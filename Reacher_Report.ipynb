{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repo proposes a solution to Continuous Control project (Version 2 of the environment, with 20 agents), which adopts DDPG Agent. DDPG uses an actor body and a critic body. The actor act to environment based on a local policy network. The critic gives the actor/policy feedbacks by evaluating the state-action value, and give guidance to the actor on how it should improve the policy. The agent has local critic and actor, and also uses target critic and actor, in order to stablize the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters used in this solution are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory buffer,  1000000\n",
      "Batch size,  128\n",
      "5 update(s) per step\n",
      "Discount rate,  0.99\n",
      "Actor learning rate,  0.0005\n",
      "Critic learning rate,  0.0005\n",
      "Soft update TAU,  0.001\n"
     ]
    }
   ],
   "source": [
    "print('Memory buffer, ', MEMORY_BUFFER)\n",
    "print('Batch size, ', BATCH_SIZE)\n",
    "print('%d update(s) per step' % UPDATE_FREQUENCY_PER_STEP)\n",
    "print('Discount rate, ', GAMMA)\n",
    "print('Actor learning rate, ', LR_ACTOR)\n",
    "print('Critic learning rate, ', LR_CRITIC)\n",
    "print('Soft update TAU, ', TAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The replay buffer has a buffer size of 1000000, and randomly sample 128 experiences at each step.\n",
    "- The agent collect 20 data points at each step, and add these experiences to memory.\n",
    "- The agent runs 5 udpates per step.\n",
    "- At each update, the agent randomly samples 128 past experiences from memory, and updates local actor and critic based on the sampled experience.\n",
    "- The actor and the critic use same learning rate, which is 0.0005.\n",
    "- The agent soft-update target actor and target critic everytime after local actor and local critic get updated, using a $\\tau$ of 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows how policy improves during training. The agent hits a score of 30 within 50 episodes, and stablized around 38 after 75 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ddpg_train_scores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved actor net, and run agent against environment in test mode for 10 episodes. The actor is able to achieve scores higher than 35 in all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from ddpg_agent import DDPGAgent\n",
    "from env_wrapper import EnvWrapper\n",
    "from utils import test_agent, plot_scores, array_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(train_mode=False)\n",
    "env = EnvWrapper(file_name='Reacher_Windows_x86_64/Reacher.exe', train_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor.load_state_dict(torch.load('checkpoints/reacher_ddpg_actor_checkpoint.pth'))\n",
    "agent.critic.load_state_dict(torch.load('checkpoints/reacher_ddpg_critic_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, avg score 37.76\n",
      "Episode 2, avg score 38.54\n",
      "Episode 3, avg score 38.36\n",
      "Episode 4, avg score 37.95\n",
      "Episode 5, avg score 38.38\n",
      "Episode 6, avg score 38.16\n",
      "Episode 7, avg score 38.25\n",
      "Episode 8, avg score 38.65\n",
      "Episode 9, avg score 37.96\n",
      "Episode 10, avg score 38.17\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "test_episodes = 10\n",
    "\n",
    "for e in range(1, test_episodes+1):\n",
    "\n",
    "    agent.reset()\n",
    "    env.reset()\n",
    "    agent.states = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done: \n",
    "        states = array_to_tensor(agent.states)\n",
    "\n",
    "        agent.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = agent.actor(states)\n",
    "            agent.actions = actions.cpu().data.numpy()\n",
    "            \n",
    "        agent.rewards, agent.next_states, agent.dones = env.step(agent.actions)\n",
    "        agent.scores += agent.rewards\n",
    "        agent.step_count += 1\n",
    "        agent.states = agent.next_states\n",
    "        done = any(agent.dones)\n",
    "    \n",
    "    scores.append(agent.scores.mean())\n",
    "\n",
    "    print('Episode %d, avg score %.2f' % (e, agent.scores.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
